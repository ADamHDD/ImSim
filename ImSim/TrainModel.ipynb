{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Constants used in training\n",
    "BatchSize = 256\n",
    "NbrNegatives = 100\n",
    "Device=\"/gpu:0\"\n",
    "\n",
    "# How much to scale the cosine similarity before applying softmax\n",
    "SoftmaxScaling = 5\n",
    "\n",
    "w2v_training_path = './Data/w2v_train2014.out.npy'\n",
    "exemplar_training_path = './Data/exemplar_train2014.out.npy'\n",
    "\n",
    "w2v_validation_path = './Data/w2v_val2014.out.npy'\n",
    "exemplar_validation_path = './Data/exemplar_val2014.out.npy'\n",
    "\n",
    "model_save_path = \"./model/sess_stored_old_training\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import tensorflow as tf\n",
    "import random\n",
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total number of training examples: 414073\n",
      "Shape of exemplar data: (414073, 1024)\n",
      "Shape of w2v data: (414073, 300)\n",
      "Total number of validation examples: 202654\n",
      "Shape of exemplar validation data: (202654, 1024)\n",
      "Shape of w2v validation data: (202654, 300)\n"
     ]
    }
   ],
   "source": [
    "# Load training data as numpy arrays into data set\n",
    "w2v_train = np.load(w2v_training_path)\n",
    "exemplar_train = np.load(exemplar_training_path)\n",
    "\n",
    "nbr_images = len(exemplar_train)\n",
    "nbr_batches = nbr_images // BatchSize\n",
    "#w2v_train = w2v_train[0:nbr_batches*BatchSize, ]\n",
    "#exemplar_train = exemplar_train[0:nbr_batches*BatchSize,]\n",
    "\n",
    "# Convert to float32 in case training data is np.float64\n",
    "w2v_train = w2v_train.astype(np.float32)\n",
    "exemplar_train = exemplar_train.astype(np.float32)\n",
    "\n",
    "print(\"Total number of training examples: \" + str(nbr_images))\n",
    "print(\"Shape of exemplar data: \" + str(exemplar_train.shape))\n",
    "print(\"Shape of w2v data: \" + str(w2v_train.shape))\n",
    "\n",
    "# Load validation data\n",
    "w2v_val = np.load(w2v_validation_path).astype(np.float32)\n",
    "exemplar_val = np.load(exemplar_validation_path).astype(np.float32)\n",
    "\n",
    "nbr_images_val = len(exemplar_val)\n",
    "print(\"Total number of validation examples: \" + str(nbr_images_val))\n",
    "print(\"Shape of exemplar validation data: \" + str(exemplar_val.shape))\n",
    "print(\"Shape of w2v validation data: \" + str(w2v_val.shape))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Take a tensor and tile it vertically NbrNegative times, with random shuffle (by rotation) in between each tiling\n",
    "def create_negatives(y):\n",
    "    with tf.device(Device):\n",
    "\n",
    "        temp = tf.tile(y, [1, 1])\n",
    "\n",
    "        for i in range(NbrNegatives):\n",
    "            rand = int((random.random() + i) * BatchSize / NbrNegatives)\n",
    "            left = tf.slice(temp, [rand, 0], [BatchSize - rand, -1])\n",
    "            right = tf.slice(temp, [0, 0], [rand, -1])\n",
    "            y = tf.concat([y, left, right], axis=0)\n",
    "        return y"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Create the cosine similarity between a batch of positiva and a tiled batch of negatives\n",
    "def create_cosine_similarity(query_y, doc_y, scaling):\n",
    "    with tf.device(Device):\n",
    "        print(query_y)\n",
    "        print(doc_y)\n",
    "        print(\"norms:\")\n",
    "        query_norm = tf.tile(tf.sqrt(tf.reduce_sum(tf.square(query_y), 1, True)), [NbrNegatives + 1, 1])\n",
    "        doc_norm = tf.sqrt(tf.reduce_sum(tf.square(doc_y), 1, True))\n",
    "        print(query_norm)\n",
    "        print(doc_norm)\n",
    "\n",
    "        prod = tf.multiply(tf.tile(query_y, [NbrNegatives + 1, 1]), doc_y)\n",
    "        print(\"Prod:\")\n",
    "        print(prod)\n",
    "        norm_prod = tf.multiply(query_norm, doc_norm)\n",
    "        print(\"Norm prod:\")\n",
    "        print(norm_prod)\n",
    "        norm_sum = tf.reduce_sum(prod, 1, True)\n",
    "        print(\"Norm sum\")\n",
    "        print(norm_sum)\n",
    "        cos_sim_raw = tf.truediv(norm_sum, norm_prod)\n",
    "        \n",
    "        print(\"cosine\")\n",
    "        print(cos_sim_raw)\n",
    "        cos_sim = tf.transpose(tf.reshape(tf.transpose(cos_sim_raw), [NbrNegatives + 1, BatchSize])) * scaling\n",
    "        print(cos_sim)\n",
    "\n",
    "        return cos_sim"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Loss function\n",
    "def create_loss(cos_sim, cos_sim_not_scaled):\n",
    "    with tf.device(\"/gpu:0\"):\n",
    "        prob = tf.nn.softmax(cos_sim)\n",
    "        prob2 = cos_sim_not_scaled\n",
    "\n",
    "        hit_prob = tf.slice(prob, [0, 0], [-1, 1])\n",
    "        hit_prob2 = tf.slice(prob2, [0, 0], [-1, 1])\n",
    "\n",
    "        loss = tf.identity(-tf.reduce_sum(tf.log(hit_prob)) / BatchSize, name='loss')\n",
    "\n",
    "        loss_raw = tf.identity(tf.reduce_sum((hit_prob2)) / BatchSize, name='loss_raw')\n",
    "\n",
    "        return loss, loss_raw"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network that maps a 300-dimensional Word2Vec vector into the new similarity vector space\n",
    "def create_w2v_network(input, is_training):\n",
    "    with tf.device(Device):\n",
    "        l1 = tf.layers.dropout(tf.layers.dense(name='w2v_l1', inputs=input, units=300, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), rate = 0.3, training=is_training)\n",
    "        l2 = tf.layers.dropout(tf.layers.dense(name='w2v_l2', inputs=l1, units=250, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), rate=0.3, training=is_training)\n",
    "        l3 = tf.layers.dense(name='w2v_l3', inputs=l2, units=200, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        return tf.layers.batch_normalization(l3, axis=1, training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create network that maps a 300-dimensional Exemplar vector into the new similarity vector space\n",
    "def create_exemplar_network(input, is_training):\n",
    "    with tf.device(Device):\n",
    "        l1 = tf.layers.dropout(tf.layers.dense(name='exemplar_l1', inputs=input, units=800, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), rate=0.4, training=is_training)\n",
    "        l2 = tf.layers.dropout(tf.layers.dense(name='exemplar_l2', inputs=l1, units=500, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), rate=0.2, training=is_training)\n",
    "        l3 = tf.layers.dropout(tf.layers.dense(name='exemplar_l3', inputs=l2, units=350, activation=tf.nn.relu, kernel_initializer=tf.contrib.layers.xavier_initializer()), rate=0.2, training=is_training)\n",
    "        l4 = tf.layers.dense(name='exemplar_l4', inputs=l3, units=200, activation=None, kernel_initializer=tf.contrib.layers.xavier_initializer())\n",
    "\n",
    "        return tf.layers.batch_normalization(l4, axis=1, training=is_training)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Tensor(\"model/batch_normalization/batchnorm/add_1:0\", shape=(?, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"concat_99:0\", shape=(?, 200), dtype=float32, device=/device:GPU:0)\n",
      "norms:\n",
      "Tensor(\"Tile_1:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Sqrt_1:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Prod:\n",
      "Tensor(\"Mul:0\", shape=(?, 200), dtype=float32, device=/device:GPU:0)\n",
      "Norm prod:\n",
      "Tensor(\"Mul_1:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Norm sum\n",
      "Tensor(\"Sum_2:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "cosine\n",
      "Tensor(\"truediv:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"mul_2:0\", shape=(256, 101), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"model/batch_normalization/batchnorm/add_1:0\", shape=(?, 200), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"concat_99:0\", shape=(?, 200), dtype=float32, device=/device:GPU:0)\n",
      "norms:\n",
      "Tensor(\"Tile_3:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"Sqrt_3:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Prod:\n",
      "Tensor(\"Mul_3:0\", shape=(?, 200), dtype=float32, device=/device:GPU:0)\n",
      "Norm prod:\n",
      "Tensor(\"Mul_4:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Norm sum\n",
      "Tensor(\"Sum_5:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "cosine\n",
      "Tensor(\"truediv_1:0\", shape=(?, 1), dtype=float32, device=/device:GPU:0)\n",
      "Tensor(\"mul_5:0\", shape=(256, 101), dtype=float32, device=/device:GPU:0)\n"
     ]
    }
   ],
   "source": [
    "#Create the network graph\n",
    "tf.reset_default_graph()\n",
    "\n",
    "with tf.device(Device):\n",
    "    # Placeholders for the input dataset\n",
    "    exemplar_placeholder = tf.placeholder(exemplar_train.dtype, [None, 1024], name='exemplar_placeholder')\n",
    "    w2v_placeholder = tf.placeholder(w2v_train.dtype, [None, 300])\n",
    "\n",
    "    # Setup datasets for training and prediction\n",
    "    dataset = tf.data.Dataset.from_tensor_slices((exemplar_placeholder, w2v_placeholder)).shuffle(buffer_size=nbr_images).repeat().batch(BatchSize) #shuffle repeat\n",
    "    dataset_val = tf.data.Dataset.from_tensor_slices((exemplar_placeholder, w2v_placeholder)).shuffle(buffer_size=nbr_images_val).repeat().batch(BatchSize) #shuffle repeat\n",
    "\n",
    "    dataset_pred = tf.data.Dataset.from_tensor_slices((exemplar_placeholder)).batch(BatchSize)\n",
    "\n",
    "    handle = tf.placeholder(tf.string, shape=[])\n",
    "    iterator = tf.data.Iterator.from_string_handle(handle, dataset.output_types, dataset.output_shapes)\n",
    "\n",
    "    # input for training\n",
    "    iter = dataset.make_initializable_iterator()\n",
    "    iter_val = dataset_val.make_initializable_iterator()\n",
    "    \n",
    "    x_exemplar, x_w2v = iterator.get_next()\n",
    "    \n",
    "    #input for prediction\n",
    "    iter_pred = dataset_pred.make_initializable_iterator(shared_name='apa')\n",
    "    x_exemplar_pred = iter_pred.get_next()\n",
    "\n",
    "    #define outputs from w2v network\n",
    "    y_w2v = create_w2v_network(x_w2v, is_training=True)\n",
    "    \n",
    "    # output from exemplar network\n",
    "    with tf.variable_scope(\"model\", reuse=False):\n",
    "        y_exemplar = create_exemplar_network(x_exemplar, True)\n",
    "        \n",
    "    # output from exemplar network for prediction (i.e. no dropout applied)\n",
    "    with tf.variable_scope(\"model\", reuse=True):\n",
    "        y_exemplar_pred = tf.identity(create_exemplar_network(exemplar_placeholder, False), name='y_exemplar_pred')\n",
    "\n",
    "    #create tensor of w2v output with negatives\n",
    "    y_negatives = create_negatives(y_w2v)\n",
    "\n",
    "    #define loss\n",
    "    cos_similarity = create_cosine_similarity(y_exemplar, y_negatives, SoftmaxScaling)\n",
    "    cos_similarity_unscaled = create_cosine_similarity(y_exemplar, y_negatives, 1)\n",
    "\n",
    "    #create loss\n",
    "    loss, loss_raw = create_loss(cos_similarity, cos_similarity_unscaled)\n",
    "\n",
    "    #define optimizer\n",
    "    train_step = tf.train.AdamOptimizer(name='train_step').minimize(loss)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "#initialize tensorflow\n",
    "init = tf.global_variables_initializer()\n",
    "sess = tf.Session(config=tf.ConfigProto(allow_soft_placement=True))\n",
    "sess.run(init)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create saver\n",
    "saver = tf.train.Saver()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Feed the training data to the dataset\n",
    "sess.run(iter.initializer, feed_dict={exemplar_placeholder: exemplar_train, w2v_placeholder: w2v_train})\n",
    "sess.run(iter_val.initializer, feed_dict={exemplar_placeholder: exemplar_val, w2v_placeholder: w2v_val})\n",
    "\n",
    "training_handle = sess.run(iter.string_handle())\n",
    "validation_handle = sess.run(iter_val.string_handle())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training 1617 batches\n",
      "\n",
      "Validation loss over 100 batches: -0.0008369434965425171\n",
      "\n",
      "Epoch 0\n",
      "Raw loss: 0.0009663295\n",
      "Validation loss over 5 batches: 0.009247484849765897\n",
      "\n",
      "Raw loss: 0.6915869\n",
      "Validation loss over 5 batches: 0.6913704514503479\n",
      "\n",
      "Raw loss: 0.73022103\n",
      "Validation loss over 5 batches: 0.7242599368095398\n",
      "\n",
      "Raw loss: 0.7536123\n",
      "Validation loss over 5 batches: 0.7338937759399414\n",
      "\n",
      "Raw loss: 0.76469576\n",
      "Validation loss over 5 batches: 0.741386330127716\n",
      "\n",
      "Raw loss: 0.7284039\n",
      "Validation loss over 5 batches: 0.7557746171951294\n",
      "\n",
      "Raw loss: 0.75618553\n",
      "Validation loss over 5 batches: 0.747964596748352\n",
      "\n",
      "Raw loss: 0.7446423\n",
      "Validation loss over 5 batches: 0.7554044604301453\n",
      "\n",
      "Raw loss: 0.7406895\n",
      "Validation loss over 5 batches: 0.7523503065109253\n",
      "\n",
      "Raw loss: 0.770108\n",
      "Validation loss over 5 batches: 0.7534955501556396\n",
      "\n",
      "Raw loss: 0.7443092\n",
      "Validation loss over 5 batches: 0.7454350352287292\n",
      "\n",
      "\n",
      "Time for epoch: 0:03:50.724578\n",
      "\n",
      "Validation loss over 100 batches: 0.7466109448671341\n",
      "\n",
      "Epoch 1\n",
      "Raw loss: 0.75785667\n",
      "Validation loss over 5 batches: 0.7449609160423278\n",
      "\n",
      "Raw loss: 0.7894379\n",
      "Validation loss over 5 batches: 0.7476606011390686\n",
      "\n",
      "Raw loss: 0.76567996\n",
      "Validation loss over 5 batches: 0.7445579051971436\n",
      "\n",
      "Raw loss: 0.74888754\n",
      "Validation loss over 5 batches: 0.7418773412704468\n",
      "\n",
      "Raw loss: 0.7768736\n",
      "Validation loss over 5 batches: 0.7437125205993652\n",
      "\n",
      "Raw loss: 0.73182595\n",
      "Validation loss over 5 batches: 0.7532467842102051\n",
      "\n",
      "Raw loss: 0.7339916\n",
      "Validation loss over 5 batches: 0.7512412190437316\n",
      "\n",
      "Raw loss: 0.7697092\n",
      "Validation loss over 5 batches: 0.7477069020271301\n",
      "\n",
      "Raw loss: 0.740459\n",
      "Validation loss over 5 batches: 0.7542009115219116\n",
      "\n",
      "Raw loss: 0.7264151\n",
      "Validation loss over 5 batches: 0.7510100603103638\n",
      "\n",
      "Raw loss: 0.76139873\n",
      "Validation loss over 5 batches: 0.7512572169303894\n",
      "\n",
      "\n",
      "Time for epoch: 0:03:51.981280\n",
      "\n",
      "Validation loss over 100 batches: 0.7456463915109635\n",
      "\n",
      "Time for training: 0:07:59.973658\n"
     ]
    }
   ],
   "source": [
    "# Compute colossst for validation set\n",
    "def run_validation(count):\n",
    "    sum = 0.0\n",
    "    for i in range(0, count):\n",
    "        sum = sum + sess.run(loss_raw,  feed_dict={handle: validation_handle})\n",
    "    sum = sum / count\n",
    "    print(\"Validation loss over \" + str(count) +  \" batches: \" + str(sum))\n",
    "    print()\n",
    "    \n",
    "#Training loop\n",
    "\n",
    "NbrEpochs = 2\n",
    "PrintoutsPerEpoch = 10\n",
    "\n",
    "batches_per_epoch = nbr_batches\n",
    "batches_to_print = batches_per_epoch // PrintoutsPerEpoch\n",
    "\n",
    "print(\"Training \" + str(nbr_batches) + \" batches\\r\\n\")\n",
    "\n",
    "run_validation(100)\n",
    "\n",
    "trainingStartTime = datetime.now()\n",
    "\n",
    "for epoch in range(NbrEpochs):\n",
    "    print(\"Epoch \" + str(epoch))\n",
    "    \n",
    "    epochStartTime = datetime.now()\n",
    "    \n",
    "    for j in range(batches_per_epoch):\n",
    "        if j % batches_to_print == 0:\n",
    "            a,b = sess.run([train_step, loss_raw],  feed_dict={handle: training_handle})\n",
    "            print(\"Raw loss: \" + str(b))\n",
    "            \n",
    "            run_validation(5)\n",
    "        else:\n",
    "            sess.run(train_step,  feed_dict={handle: training_handle})\n",
    "            \n",
    "    print()\n",
    "    print(\"Time for epoch:\", datetime.now() - epochStartTime)\n",
    "    print()\n",
    "    run_validation(100)\n",
    "    \n",
    "print(\"Time for training:\", datetime.now() - trainingStartTime)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'./model/sess_stored_old_training'"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "saver.save(sess, model_save_path)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#tensorboard"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#rank metric"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python [conda env:tfgpu]",
   "language": "python",
   "name": "conda-env-tfgpu-py"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.5.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
